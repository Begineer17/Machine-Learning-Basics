{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # Visualization\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch # Library for implementing Deep Neural Network\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "from copy import deepcopy as dc\n",
        "import math\n",
        "# import ray\n",
        "# from ray import tune\n",
        "# from ray.tune.schedulers import ASHAScheduler\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Setup device-agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AtY66z4KE9Y5",
        "outputId": "95e47b0a-b1e6-46ea-b2ff-a0e3df044761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Apple.Inc Stock Data\n",
        "import yfinance as yf\n",
        "from datetime import date, timedelta, datetime\n",
        "\n",
        "end_date = date.today().strftime(\"%Y-%m-%d\") #end date for our data retrieval will be current date\n",
        "start_date = '1990-01-01' # Beginning date for our historical data retrieval\n",
        "\n",
        "df = yf.download('AAPL', start=start_date, end=end_date)# Function used to fetch the data\n",
        "\n",
        "data = df[[\"Close\"]]\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "jyebikz5FEqn",
        "outputId": "525de8e5-c26d-4d7a-e4e7-716bdf1b243a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Close\n",
              "Date                  \n",
              "1990-01-02    0.332589\n",
              "1990-01-03    0.334821\n",
              "1990-01-04    0.335938\n",
              "1990-01-05    0.337054\n",
              "1990-01-08    0.339286\n",
              "...                ...\n",
              "2024-06-12  213.070007\n",
              "2024-06-13  214.240005\n",
              "2024-06-14  212.490005\n",
              "2024-06-17  216.669998\n",
              "2024-06-18  214.289993\n",
              "\n",
              "[8682 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-267609c9-4428-440d-9fd1-b48c5cbf7d84\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1990-01-02</th>\n",
              "      <td>0.332589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1990-01-03</th>\n",
              "      <td>0.334821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1990-01-04</th>\n",
              "      <td>0.335938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1990-01-05</th>\n",
              "      <td>0.337054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1990-01-08</th>\n",
              "      <td>0.339286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-06-12</th>\n",
              "      <td>213.070007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-06-13</th>\n",
              "      <td>214.240005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-06-14</th>\n",
              "      <td>212.490005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-06-17</th>\n",
              "      <td>216.669998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-06-18</th>\n",
              "      <td>214.289993</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8682 rows × 1 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-267609c9-4428-440d-9fd1-b48c5cbf7d84')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-267609c9-4428-440d-9fd1-b48c5cbf7d84 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-267609c9-4428-440d-9fd1-b48c5cbf7d84');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-45191ae9-45fb-464b-9c54-3689e704bcc4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-45191ae9-45fb-464b-9c54-3689e704bcc4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-45191ae9-45fb-464b-9c54-3689e704bcc4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_61e374c0-ef59-425f-9d14-f8f0666cae35\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_61e374c0-ef59-425f-9d14-f8f0666cae35 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 8682,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1990-01-02 00:00:00\",\n        \"max\": \"2024-06-18 00:00:00\",\n        \"num_unique_values\": 8682,\n        \"samples\": [\n          \"2005-09-27 00:00:00\",\n          \"1996-08-09 00:00:00\",\n          \"2002-02-14 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 49.004589469343756,\n        \"min\": 0.11551299691200256,\n        \"max\": 216.6699981689453,\n        \"num_unique_values\": 6222,\n        \"samples\": [\n          0.15792399644851685,\n          16.703929901123047,\n          123.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataframe_for_lstm(df, n_steps):\n",
        "    df = dc(df)\n",
        "\n",
        "    for i in range(1, n_steps+1):\n",
        "        df[f'Close(t-{i})'] = df['Close'].shift(i)\n",
        "\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "lookback = 50\n",
        "shifted_df = prepare_dataframe_for_lstm(data, lookback)\n",
        "\n",
        "shifted_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "eS4oDuHnFIqJ",
        "outputId": "83dd1009-26cb-410c-802c-79997beae92a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Close  Close(t-1)  Close(t-2)  Close(t-3)  Close(t-4)  \\\n",
              "Date                                                                     \n",
              "1990-03-14    0.330357    0.329241    0.327009    0.329241    0.328125   \n",
              "1990-03-15    0.328125    0.330357    0.329241    0.327009    0.329241   \n",
              "1990-03-16    0.359375    0.328125    0.330357    0.329241    0.327009   \n",
              "1990-03-19    0.378348    0.359375    0.328125    0.330357    0.329241   \n",
              "1990-03-20    0.369420    0.378348    0.359375    0.328125    0.330357   \n",
              "...                ...         ...         ...         ...         ...   \n",
              "2024-06-12  213.070007  207.149994  193.119995  196.889999  194.479996   \n",
              "2024-06-13  214.240005  213.070007  207.149994  193.119995  196.889999   \n",
              "2024-06-14  212.490005  214.240005  213.070007  207.149994  193.119995   \n",
              "2024-06-17  216.669998  212.490005  214.240005  213.070007  207.149994   \n",
              "2024-06-18  214.289993  216.669998  212.490005  214.240005  213.070007   \n",
              "\n",
              "            Close(t-5)  Close(t-6)  Close(t-7)  Close(t-8)  Close(t-9)  ...  \\\n",
              "Date                                                                    ...   \n",
              "1990-03-14    0.315848    0.314732    0.308036    0.301339    0.305804  ...   \n",
              "1990-03-15    0.328125    0.315848    0.314732    0.308036    0.301339  ...   \n",
              "1990-03-16    0.329241    0.328125    0.315848    0.314732    0.308036  ...   \n",
              "1990-03-19    0.327009    0.329241    0.328125    0.315848    0.314732  ...   \n",
              "1990-03-20    0.329241    0.327009    0.329241    0.328125    0.315848  ...   \n",
              "...                ...         ...         ...         ...         ...  ...   \n",
              "2024-06-12  195.869995  194.350006  194.029999  192.250000  191.289993  ...   \n",
              "2024-06-13  194.479996  195.869995  194.350006  194.029999  192.250000  ...   \n",
              "2024-06-14  196.889999  194.479996  195.869995  194.350006  194.029999  ...   \n",
              "2024-06-17  193.119995  196.889999  194.479996  195.869995  194.350006  ...   \n",
              "2024-06-18  207.149994  193.119995  196.889999  194.479996  195.869995  ...   \n",
              "\n",
              "            Close(t-41)  Close(t-42)  Close(t-43)  Close(t-44)  Close(t-45)  \\\n",
              "Date                                                                          \n",
              "1990-03-14     0.305804     0.308036     0.308036     0.321429     0.335938   \n",
              "1990-03-15     0.311384     0.305804     0.308036     0.308036     0.321429   \n",
              "1990-03-16     0.296875     0.311384     0.305804     0.308036     0.308036   \n",
              "1990-03-19     0.289063     0.296875     0.311384     0.305804     0.308036   \n",
              "1990-03-20     0.305804     0.289063     0.296875     0.311384     0.305804   \n",
              "...                 ...          ...          ...          ...          ...   \n",
              "2024-06-12   172.690002   176.550003   175.039993   167.779999   169.669998   \n",
              "2024-06-13   169.380005   172.690002   176.550003   175.039993   167.779999   \n",
              "2024-06-14   168.000000   169.380005   172.690002   176.550003   175.039993   \n",
              "2024-06-17   167.039993   168.000000   169.380005   172.690002   176.550003   \n",
              "2024-06-18   165.000000   167.039993   168.000000   169.380005   172.690002   \n",
              "\n",
              "            Close(t-46)  Close(t-47)  Close(t-48)  Close(t-49)  Close(t-50)  \n",
              "Date                                                                         \n",
              "1990-03-14     0.339286     0.337054     0.335938     0.334821     0.332589  \n",
              "1990-03-15     0.335938     0.339286     0.337054     0.335938     0.334821  \n",
              "1990-03-16     0.321429     0.335938     0.339286     0.337054     0.335938  \n",
              "1990-03-19     0.308036     0.321429     0.335938     0.339286     0.337054  \n",
              "1990-03-20     0.308036     0.308036     0.321429     0.335938     0.339286  \n",
              "...                 ...          ...          ...          ...          ...  \n",
              "2024-06-12   168.449997   169.580002   168.820007   169.649994   168.839996  \n",
              "2024-06-13   169.669998   168.449997   169.580002   168.820007   169.649994  \n",
              "2024-06-14   167.779999   169.669998   168.449997   169.580002   168.820007  \n",
              "2024-06-17   175.039993   167.779999   169.669998   168.449997   169.580002  \n",
              "2024-06-18   176.550003   175.039993   167.779999   169.669998   168.449997  \n",
              "\n",
              "[8632 rows x 51 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a3e98e59-f0b3-44c8-bf53-d74e7916cf6c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close</th>\n",
              "      <th>Close(t-1)</th>\n",
              "      <th>Close(t-2)</th>\n",
              "      <th>Close(t-3)</th>\n",
              "      <th>Close(t-4)</th>\n",
              "      <th>Close(t-5)</th>\n",
              "      <th>Close(t-6)</th>\n",
              "      <th>Close(t-7)</th>\n",
              "      <th>Close(t-8)</th>\n",
              "      <th>Close(t-9)</th>\n",
              "      <th>...</th>\n",
              "      <th>Close(t-41)</th>\n",
              "      <th>Close(t-42)</th>\n",
              "      <th>Close(t-43)</th>\n",
              "      <th>Close(t-44)</th>\n",
              "      <th>Close(t-45)</th>\n",
              "      <th>Close(t-46)</th>\n",
              "      <th>Close(t-47)</th>\n",
              "      <th>Close(t-48)</th>\n",
              "      <th>Close(t-49)</th>\n",
              "      <th>Close(t-50)</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1990-03-14</th>\n",
              "      <td>0.330357</td>\n",
              "      <td>0.329241</td>\n",
              "      <td>0.327009</td>\n",
              "      <td>0.329241</td>\n",
              "      <td>0.328125</td>\n",
              "      <td>0.315848</td>\n",
              "      <td>0.314732</td>\n",
              "      <td>0.308036</td>\n",
              "      <td>0.301339</td>\n",
              "      <td>0.305804</td>\n",
              "      <td>...</td>\n",
              "      <td>0.305804</td>\n",
              "      <td>0.308036</td>\n",
              "      <td>0.308036</td>\n",
              "      <td>0.321429</td>\n",
              "      <td>0.335938</td>\n",
              "      <td>0.339286</td>\n",
              "      <td>0.337054</td>\n",
              "      <td>0.335938</td>\n",
              "      <td>0.334821</td>\n",
              "      <td>0.332589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1990-03-15</th>\n",
              "      <td>0.328125</td>\n",
              "      <td>0.330357</td>\n",
              "      <td>0.329241</td>\n",
              "      <td>0.327009</td>\n",
              "      <td>0.329241</td>\n",
              "      <td>0.328125</td>\n",
              "      <td>0.315848</td>\n",
              "      <td>0.314732</td>\n",
              "      <td>0.308036</td>\n",
              "      <td>0.301339</td>\n",
              "      <td>...</td>\n",
              "      <td>0.311384</td>\n",
              "      <td>0.305804</td>\n",
              "      <td>0.308036</td>\n",
              "      <td>0.308036</td>\n",
              "      <td>0.321429</td>\n",
              "      <td>0.335938</td>\n",
              "      <td>0.339286</td>\n",
              "      <td>0.337054</td>\n",
              "      <td>0.335938</td>\n",
              "      <td>0.334821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1990-03-16</th>\n",
              "      <td>0.359375</td>\n",
              "      <td>0.328125</td>\n",
              "      <td>0.330357</td>\n",
              "      <td>0.329241</td>\n",
              "      <td>0.327009</td>\n",
              "      <td>0.329241</td>\n",
              "      <td>0.328125</td>\n",
              "      <td>0.315848</td>\n",
              "      <td>0.314732</td>\n",
              "      <td>0.308036</td>\n",
              "      <td>...</td>\n",
              "      <td>0.296875</td>\n",
              "      <td>0.311384</td>\n",
              "      <td>0.305804</td>\n",
              "      <td>0.308036</td>\n",
              "      <td>0.308036</td>\n",
              "      <td>0.321429</td>\n",
              "      <td>0.335938</td>\n",
              "      <td>0.339286</td>\n",
              "      <td>0.337054</td>\n",
              "      <td>0.335938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1990-03-19</th>\n",
              "      <td>0.378348</td>\n",
              "      <td>0.359375</td>\n",
              "      <td>0.328125</td>\n",
              "      <td>0.330357</td>\n",
              "      <td>0.329241</td>\n",
              "      <td>0.327009</td>\n",
              "      <td>0.329241</td>\n",
              "      <td>0.328125</td>\n",
              "      <td>0.315848</td>\n",
              "      <td>0.314732</td>\n",
              "      <td>...</td>\n",
              "      <td>0.289063</td>\n",
              "      <td>0.296875</td>\n",
              "      <td>0.311384</td>\n",
              "      <td>0.305804</td>\n",
              "      <td>0.308036</td>\n",
              "      <td>0.308036</td>\n",
              "      <td>0.321429</td>\n",
              "      <td>0.335938</td>\n",
              "      <td>0.339286</td>\n",
              "      <td>0.337054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1990-03-20</th>\n",
              "      <td>0.369420</td>\n",
              "      <td>0.378348</td>\n",
              "      <td>0.359375</td>\n",
              "      <td>0.328125</td>\n",
              "      <td>0.330357</td>\n",
              "      <td>0.329241</td>\n",
              "      <td>0.327009</td>\n",
              "      <td>0.329241</td>\n",
              "      <td>0.328125</td>\n",
              "      <td>0.315848</td>\n",
              "      <td>...</td>\n",
              "      <td>0.305804</td>\n",
              "      <td>0.289063</td>\n",
              "      <td>0.296875</td>\n",
              "      <td>0.311384</td>\n",
              "      <td>0.305804</td>\n",
              "      <td>0.308036</td>\n",
              "      <td>0.308036</td>\n",
              "      <td>0.321429</td>\n",
              "      <td>0.335938</td>\n",
              "      <td>0.339286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-06-12</th>\n",
              "      <td>213.070007</td>\n",
              "      <td>207.149994</td>\n",
              "      <td>193.119995</td>\n",
              "      <td>196.889999</td>\n",
              "      <td>194.479996</td>\n",
              "      <td>195.869995</td>\n",
              "      <td>194.350006</td>\n",
              "      <td>194.029999</td>\n",
              "      <td>192.250000</td>\n",
              "      <td>191.289993</td>\n",
              "      <td>...</td>\n",
              "      <td>172.690002</td>\n",
              "      <td>176.550003</td>\n",
              "      <td>175.039993</td>\n",
              "      <td>167.779999</td>\n",
              "      <td>169.669998</td>\n",
              "      <td>168.449997</td>\n",
              "      <td>169.580002</td>\n",
              "      <td>168.820007</td>\n",
              "      <td>169.649994</td>\n",
              "      <td>168.839996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-06-13</th>\n",
              "      <td>214.240005</td>\n",
              "      <td>213.070007</td>\n",
              "      <td>207.149994</td>\n",
              "      <td>193.119995</td>\n",
              "      <td>196.889999</td>\n",
              "      <td>194.479996</td>\n",
              "      <td>195.869995</td>\n",
              "      <td>194.350006</td>\n",
              "      <td>194.029999</td>\n",
              "      <td>192.250000</td>\n",
              "      <td>...</td>\n",
              "      <td>169.380005</td>\n",
              "      <td>172.690002</td>\n",
              "      <td>176.550003</td>\n",
              "      <td>175.039993</td>\n",
              "      <td>167.779999</td>\n",
              "      <td>169.669998</td>\n",
              "      <td>168.449997</td>\n",
              "      <td>169.580002</td>\n",
              "      <td>168.820007</td>\n",
              "      <td>169.649994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-06-14</th>\n",
              "      <td>212.490005</td>\n",
              "      <td>214.240005</td>\n",
              "      <td>213.070007</td>\n",
              "      <td>207.149994</td>\n",
              "      <td>193.119995</td>\n",
              "      <td>196.889999</td>\n",
              "      <td>194.479996</td>\n",
              "      <td>195.869995</td>\n",
              "      <td>194.350006</td>\n",
              "      <td>194.029999</td>\n",
              "      <td>...</td>\n",
              "      <td>168.000000</td>\n",
              "      <td>169.380005</td>\n",
              "      <td>172.690002</td>\n",
              "      <td>176.550003</td>\n",
              "      <td>175.039993</td>\n",
              "      <td>167.779999</td>\n",
              "      <td>169.669998</td>\n",
              "      <td>168.449997</td>\n",
              "      <td>169.580002</td>\n",
              "      <td>168.820007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-06-17</th>\n",
              "      <td>216.669998</td>\n",
              "      <td>212.490005</td>\n",
              "      <td>214.240005</td>\n",
              "      <td>213.070007</td>\n",
              "      <td>207.149994</td>\n",
              "      <td>193.119995</td>\n",
              "      <td>196.889999</td>\n",
              "      <td>194.479996</td>\n",
              "      <td>195.869995</td>\n",
              "      <td>194.350006</td>\n",
              "      <td>...</td>\n",
              "      <td>167.039993</td>\n",
              "      <td>168.000000</td>\n",
              "      <td>169.380005</td>\n",
              "      <td>172.690002</td>\n",
              "      <td>176.550003</td>\n",
              "      <td>175.039993</td>\n",
              "      <td>167.779999</td>\n",
              "      <td>169.669998</td>\n",
              "      <td>168.449997</td>\n",
              "      <td>169.580002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2024-06-18</th>\n",
              "      <td>214.289993</td>\n",
              "      <td>216.669998</td>\n",
              "      <td>212.490005</td>\n",
              "      <td>214.240005</td>\n",
              "      <td>213.070007</td>\n",
              "      <td>207.149994</td>\n",
              "      <td>193.119995</td>\n",
              "      <td>196.889999</td>\n",
              "      <td>194.479996</td>\n",
              "      <td>195.869995</td>\n",
              "      <td>...</td>\n",
              "      <td>165.000000</td>\n",
              "      <td>167.039993</td>\n",
              "      <td>168.000000</td>\n",
              "      <td>169.380005</td>\n",
              "      <td>172.690002</td>\n",
              "      <td>176.550003</td>\n",
              "      <td>175.039993</td>\n",
              "      <td>167.779999</td>\n",
              "      <td>169.669998</td>\n",
              "      <td>168.449997</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8632 rows × 51 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a3e98e59-f0b3-44c8-bf53-d74e7916cf6c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a3e98e59-f0b3-44c8-bf53-d74e7916cf6c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a3e98e59-f0b3-44c8-bf53-d74e7916cf6c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d517b8a6-a8df-4fd3-825d-cbc2836a78db\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d517b8a6-a8df-4fd3-825d-cbc2836a78db')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d517b8a6-a8df-4fd3-825d-cbc2836a78db button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f388158a-825e-4491-b403-2e2154967dfb\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('shifted_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f388158a-825e-4491-b403-2e2154967dfb button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('shifted_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "shifted_df"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shifted_df_as_numpy = shifted_df.to_numpy()\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "shifted_df_as_np = scaler.fit_transform(shifted_df_as_numpy)\n",
        "\n",
        "shifted_df_as_np\n",
        "\n",
        "X = shifted_df_as_np[:, 1:]\n",
        "y = shifted_df_as_np[:, 0]\n",
        "\n",
        "train_split = int(len(X) * 0.8)\n",
        "\n",
        "X_train = X[:train_split]\n",
        "X_test = X[train_split:]\n",
        "\n",
        "y_train = y[:train_split]\n",
        "y_test = y[train_split:]\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_Bb3d-UFN-C",
        "outputId": "341d5c6e-aebf-4082-c0df-da3a969c0ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((6905, 50), (1727, 50), (6905,), (1727,))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train.reshape((-1, lookback, 1))\n",
        "X_test = X_test.reshape((-1, lookback, 1))\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "\n",
        "y_train = y_train.reshape((-1, 1))\n",
        "y_test = y_test.reshape((-1, 1))\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP41gwAJFbNO",
        "outputId": "1cdc7b45-bbdc-4575-95e5-bfb7fa5ba195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (6905, 50, 1)\n",
            "X_test shape: (1727, 50, 1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((6905, 50, 1), (1727, 50, 1), (6905, 1), (1727, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = torch.tensor(X_train).float()\n",
        "y_train = torch.tensor(y_train).float()\n",
        "X_test = torch.tensor(X_test).float()\n",
        "y_test = torch.tensor(y_test).float()\n",
        "\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnOxDdzJGDbj",
        "outputId": "6c0beac9-353d-473f-805e-fb80529e70ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([6905, 50, 1]),\n",
              " torch.Size([1727, 50, 1]),\n",
              " torch.Size([6905, 1]),\n",
              " torch.Size([1727, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.X[i], self.y[i]\n",
        "\n",
        "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
        "test_dataset = TimeSeriesDataset(X_test, y_test)\n",
        "\n",
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l94PJwgaFfdD",
        "outputId": "31694ec4-b9fe-406d-9bef-523902ef4edc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.TimeSeriesDataset at 0x7916c2f32bc0>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3BwvvZMDr57"
      },
      "outputs": [],
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "    def __init__(self, dim_model: int):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(1, dim_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Assuming x is (batch, seq_len, 1)\n",
        "        return self.linear(x) * math.sqrt(self.linear.out_features)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, dim_model: int, seq_len: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        pe = torch.zeros(seq_len, dim_model)\n",
        "        pos = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0) / dim_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div_term)\n",
        "        pe[:, 1::2] = torch.cos(pos * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.shape[1], :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, dim_model: int, h: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.h = h\n",
        "        self.d_k = dim_model // h\n",
        "        self.w_q = nn.Linear(dim_model, dim_model, bias=False)\n",
        "        self.w_k = nn.Linear(dim_model, dim_model, bias=False)\n",
        "        self.w_v = nn.Linear(dim_model, dim_model, bias=False)\n",
        "        self.w_o = nn.Linear(dim_model, dim_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def attention(self, query, key, value, mask=None):\n",
        "        d_k = query.size(-1)\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            # Ensure mask dimensions match scores dimensions\n",
        "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        p_attn = torch.softmax(scores, dim=-1)\n",
        "        if self.dropout is not None:\n",
        "            p_attn = self.dropout(p_attn)\n",
        "        return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "        q = self.w_q(q).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        k = self.w_k(k).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        v = self.w_v(v).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        x, self.attn = self.attention(q, k, v, mask)\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
        "\n",
        "        return self.w_o(x)\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, features: int, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.ones(features))\n",
        "        self.bias = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, dim_model: int, dim_ff: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(dim_model, dim_ff)\n",
        "        self.linear_2 = nn.Linear(dim_ff, dim_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, features: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.norm = LayerNormalization(features)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, features: int, multi_head_attention: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float):\n",
        "        super().__init__()\n",
        "        self.multi_head_attention = multi_head_attention\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.multi_head_attention(x, x, x, mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList, features: int):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, features: int, self_attention_block: MultiHeadAttention, cross_attention_block: MultiHeadAttention, feed_forward_block: FeedForwardBlock, dropout: float):\n",
        "        super().__init__()\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "\n",
        "    # def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "    def forward(self, x, encoder_output, src_mask):\n",
        "        # x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x))\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList, features: int):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    # def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "    def forward(self, x, encoder_output, src_mask):\n",
        "        for layer in self.layers:\n",
        "            # x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "            x = layer(x, encoder_output, src_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, d_model, output_size) -> None:\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(d_model, output_size)\n",
        "\n",
        "    def forward(self, x) -> None:\n",
        "        return self.proj(x)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbedding, tgt_embed: InputEmbedding, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    # def encode(self, src, src_mask):\n",
        "    #     src = self.src_embed(src)\n",
        "    #     src = self.src_pos(src)\n",
        "    #     return self.encoder(src, src_mask)\n",
        "\n",
        "    # def decode(self, tgt, encoder_output, src_mask, tgt_mask):\n",
        "    #     tgt = self.tgt_embed(tgt)\n",
        "    #     tgt = self.tgt_pos(tgt)\n",
        "    #     return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    # def forward(self, src, src_mask, tgt, tgt_mask):\n",
        "    def forward(self, src, src_mask, tgt):\n",
        "      src = self.src_embed(src)\n",
        "      src = self.src_pos(src)\n",
        "      encoder_output = self.encoder(src, src_mask)\n",
        "\n",
        "      # Reshape tgt to (batch, 1, 1) before passing to tgt_embed\n",
        "      tgt = tgt.unsqueeze(1).unsqueeze(2)  # Add two extra dimensions\n",
        "\n",
        "      tgt = self.tgt_embed(tgt)\n",
        "      # tgt = self.tgt_pos(tgt)\n",
        "      # output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "      output = self.decoder(tgt, encoder_output, src_mask)\n",
        "\n",
        "      # output = output.view(output.size(0), -1) # Reshape output to (batch_size, dim_model) before projection\n",
        "\n",
        "      return self.projection_layer(output)\n",
        "\n",
        "def build_transformer(seq_len: int, dim_model: int=512, N: int=6, h: int=8, dropout: float=0.1, dim_ff: int=2048, output_size: int=1) -> Transformer:\n",
        "    src_embed = InputEmbedding(dim_model)\n",
        "    tgt_embed = InputEmbedding(dim_model)\n",
        "\n",
        "    encoder_blocks = []\n",
        "    decoder_blocks = []\n",
        "\n",
        "    for i in range(N):\n",
        "        multi_head_attention = MultiHeadAttention(dim_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(dim_model, dim_ff, dropout)\n",
        "        encoder_blocks.append(EncoderBlock(dim_model, multi_head_attention, feed_forward_block, dropout))\n",
        "\n",
        "    encoder = Encoder(nn.ModuleList(encoder_blocks), dim_model)\n",
        "\n",
        "    for i in range(N):\n",
        "        multi_head_attention = MultiHeadAttention(dim_model, h, dropout)\n",
        "        cross_attention_block = MultiHeadAttention(dim_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(dim_model, dim_ff, dropout)\n",
        "        decoder_blocks.append(DecoderBlock(dim_model, multi_head_attention, cross_attention_block, feed_forward_block, dropout))\n",
        "\n",
        "    decoder = Decoder(nn.ModuleList(decoder_blocks), dim_model)\n",
        "\n",
        "    src_pos = PositionalEncoding(dim_model, seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(dim_model, seq_len, dropout)\n",
        "\n",
        "    projection_layer = ProjectionLayer(dim_model, output_size)\n",
        "\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
        "\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0.01)"
      ],
      "metadata": {
        "id": "dnxlWFyC1UEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_transformer(seq_len=50)\n",
        "model.to(device)\n",
        "model\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "learning_rate = 0.000001\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "test_loader"
      ],
      "metadata": {
        "id": "vkfHvu78Gnwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0a12a24-0c23-4cda-fde5-1a454ba91098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7916569f8e50>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on the diagonal.\"\"\"\n",
        "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "# Example usage in the transformer model\n",
        "def train_one_epoch():\n",
        "    model.train()\n",
        "    print(f'Epoch: {epoch + 1}')\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_index, batch in enumerate(train_loader):\n",
        "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
        "        # print(x_batch.size(1))\n",
        "        # break\n",
        "\n",
        "        # Normalize input data if not already done\n",
        "        x_batch = (x_batch - x_batch.mean(dim=0)) / x_batch.std(dim=0)\n",
        "\n",
        "        # # Generate source and target masks\n",
        "        src_mask = generate_square_subsequent_mask(x_batch.size(1)).to(device)\n",
        "        # # Ensure y_batch has a sequence length of at least 2\n",
        "        # if y_batch.size(1) > 1:\n",
        "        #     tgt_mask = generate_square_subsequent_mask(y_batch[:, :-1].size(1)).to(device)  # Adjust mask for sliced target\n",
        "        # else:\n",
        "        #     continue  # Skip this batch if target sequence is too short\n",
        "\n",
        "        # Ensure masks have the correct dimensions\n",
        "        src_mask = src_mask.unsqueeze(0).unsqueeze(1)\n",
        "        # if y_batch.size(1) > 1:\n",
        "        #     tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        optimizer.zero_grad()\n",
        "        # if y_batch.size(1) > 1:\n",
        "        output = model(x_batch, src_mask, y_batch[:, 0])\n",
        "\n",
        "        # Clamp outputs to avoid extreme values\n",
        "        output = torch.clamp(output, min=-1e9, max=1e9)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = loss_function(output, y_batch[:, 0])\n",
        "\n",
        "        if torch.isnan(loss):\n",
        "            print(\"NaN loss detected\")\n",
        "            continue\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Check for NaN in gradients\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None and torch.isnan(param.grad).any():\n",
        "                print(f\"NaN found in gradients of {name}\")\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_index % 100 == 99:  # print every 100 batches\n",
        "            print(output)\n",
        "            avg_loss_across_batches = running_loss / 100\n",
        "            print('Batch {0}, Loss: {1:.3f}'.format(batch_index + 1, avg_loss_across_batches))\n",
        "            running_loss = 0.0\n",
        "    print()\n",
        "\n",
        "def validate_one_epoch():\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_index, batch in enumerate(test_loader):\n",
        "        x_batch, y_batch = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "        # Generate source and target masks\n",
        "        src_mask = generate_square_subsequent_mask(x_batch.size(1)).to(device)\n",
        "\n",
        "        # Handle cases where y_batch has a sequence length of 1 or less\n",
        "        # if y_batch.size(1) > 1:\n",
        "        #     tgt_mask = generate_square_subsequent_mask(y_batch[:, :-1].size(1)).to(device)  # Adjust mask for sliced target\n",
        "        # else:\n",
        "        #     print(\"Warning: Skipping batch with target sequence length <= 1\") # Print a warning\n",
        "        #     continue  # Skip this batch\n",
        "\n",
        "        # Ensure masks have the correct dimensions\n",
        "        src_mask = src_mask.unsqueeze(0).unsqueeze(1)\n",
        "        # if y_batch.size(1) > 1:\n",
        "        #     tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Forward pass through the model\n",
        "            # if y_batch.size(1) > 1: # Only perform forward pass if target sequence is long enough\n",
        "            output = model(x_batch, src_mask, y_batch[:, 0])\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_function(output, y_batch[:, 0])\n",
        "            running_loss += loss.item()\n",
        "\n",
        "    # Handle the case where all batches were skipped\n",
        "    if len(test_loader) > 0:\n",
        "        avg_loss_across_batches = running_loss / len(test_loader)\n",
        "        print('Val Loss: {0:.3f}'.format(avg_loss_across_batches))\n",
        "    else:\n",
        "        print(\"Warning: No batches processed in validation due to short target sequences.\")\n",
        "    print('***************************************************')\n",
        "    print()"
      ],
      "metadata": {
        "id": "XCiahSwUG_lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    train_one_epoch()\n",
        "    validate_one_epoch()\n",
        "\n",
        "    # Check for NaN in model parameters after each epoch\n",
        "    for name, param in model.named_parameters():\n",
        "        if torch.isnan(param).any():\n",
        "            print(f\"NaN found in {name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Saf36afQHCBG",
        "outputId": "ff5f491a-693f-414b-96dc-6ba6bf4020b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 50, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n",
            "NaN loss detected\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-63410cd735d9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mvalidate_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-48468d6c46e9>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# if y_batch.size(1) > 1:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Clamp outputs to avoid extreme values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-228c67ece8c1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, tgt)\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# tgt = self.tgt_pos(tgt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0;31m# output = self.decoder(tgt, encoder_output, src_mask, tgt_mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m       \u001b[0;31m# output = output.view(output.size(0), -1) # Reshape output to (batch_size, dim_model) before projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-228c67ece8c1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoder_output, src_mask)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;31m# x = layer(x, encoder_output, src_mask, tgt_mask)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-228c67ece8c1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, encoder_output, src_mask)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual_connections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attention_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual_connections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attention_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresidual_connections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_block\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-228c67ece8c1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msublayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msublayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mEncoderBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-228c67ece8c1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mFeedForwardBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \u001b[0;31m# See full discussion on the problems with returning `Union` here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1695\u001b[0m     \u001b[0;31m# https://github.com/microsoft/pyright/issues/4213\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1696\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1697\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1698\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = model(X_test.to(device)).detach().cpu().numpy().flatten()\n",
        "\n",
        "dummies = np.zeros((X_test.shape[0], lookback+3))\n",
        "dummies[:, 0] = test_predictions\n",
        "dummies = scaler.inverse_transform(dummies)\n",
        "\n",
        "test_predictions = dc(dummies[:, 0])\n",
        "test_predictions\n",
        "\n",
        "dummies = np.zeros((X_test.shape[0], lookback+3))\n",
        "dummies[:, 0] = y_test.flatten()\n",
        "dummies = scaler.inverse_transform(dummies)\n",
        "\n",
        "new_y_test = dc(dummies[:, 0])\n",
        "new_y_test\n",
        "\n",
        "plt.plot(new_y_test, label='Actual Close')\n",
        "plt.plot(test_predictions, label='Predicted Close')\n",
        "plt.xlabel('Day')\n",
        "plt.ylabel('Close')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hGvLSmIQInYi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}