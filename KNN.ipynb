{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Concept:\n",
        "KNN operates under the assumption that similar data points should have similar labels (in classification) or similar values (in regression). It's a lazy learning algorithm because it does not explicitly learn a model during training phase. Instead, it memorizes the training dataset and makes predictions based on new data points' proximity to known data points.\n",
        "\n",
        "### Algorithm Steps:\n",
        "1. **Choose K**: Determine the number \\( K \\) of neighbors to consider.\n",
        "2. **Calculate Distance**: Compute the distance (typically Euclidean distance) between the new data point and all points in the training dataset.\n",
        "3. **Select K Neighbors**: Identify the K nearest neighbors based on the computed distances.\n",
        "4. **Majority Vote (Classification)** / **Average (Regression)**: For classification, assign the class label by majority vote among the K neighbors. For regression, predict the average of the target values of the K neighbors.\n",
        "\n",
        "### Key Parameters:\n",
        "- **K**: Number of neighbors to consider.\n",
        "- **Distance Metric**: Typically Euclidean distance, but other metrics like Manhattan distance or cosine similarity can also be used depending on the data and problem.\n",
        "\n",
        "### Advantages:\n",
        "- **Simple**: Easy to understand and implement.\n",
        "- **No Training Time**: No explicit training phase; predictions are made directly based on the training data.\n",
        "- **Versatile**: Can be used for both classification and regression tasks.\n",
        "- **Non-parametric**: Can handle complex decision boundaries.\n",
        "\n",
        "### Disadvantages:\n",
        "- **Computational Cost**: As the size of the training dataset grows, the prediction time can be slow because it requires computing distances to all training samples.\n",
        "- **Storage**: Needs to store the entire training dataset.\n",
        "- **Sensitive to Outliers**: Outliers can significantly affect the prediction, especially in small \\( K \\).\n",
        "\n",
        "### Practical Considerations:\n",
        "- **Choosing K**: Typically chosen using techniques like cross-validation to find an optimal balance between bias and variance, or square root of the number of data.\n",
        "- **Normalization**: Data normalization (scaling features) can be crucial because KNN is sensitive to the scale of features.\n",
        "- **Distance Metrics**: Selection of appropriate distance metrics can impact the algorithmâ€™s performance.\n",
        "\n",
        "### Applications:\n",
        "- **Classification**: Identifying the class of a new observation based on labeled examples.\n",
        "- **Regression**: Predicting a continuous-valued attribute based on the average of its neighbors.\n",
        "- **Anomaly Detection**: Identifying unusual data points based on their distance to their nearest neighbors."
      ],
      "metadata": {
        "id": "Aus5ChWEZGUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "import math\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the wine dataset\n",
        "X, y = datasets.load_wine(as_frame=True, return_X_y=True)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA to reduce to 3 components\n",
        "pca = PCA(n_components=3)\n",
        "X = pca.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "\n",
        "# Determine the value of k\n",
        "k = math.floor(math.sqrt(len(y_test)))\n",
        "k = k - 1 if (k % 2 == 0) else k\n",
        "\n",
        "# Priority Queue Node class\n",
        "class PriorityQueueNode:\n",
        "    def __init__(self, value, pr):\n",
        "        self.data = value\n",
        "        self.priority = pr\n",
        "        self.next = None\n",
        "\n",
        "# Priority Queue class\n",
        "class PriorityQueue:\n",
        "    def __init__(self):\n",
        "        self.front = None\n",
        "\n",
        "    def isEmpty(self):\n",
        "        return self.front is None\n",
        "\n",
        "    def push(self, value, priority):\n",
        "        newNode = PriorityQueueNode(value, priority)\n",
        "        if self.isEmpty() or self.front.priority > priority:\n",
        "            newNode.next = self.front\n",
        "            self.front = newNode\n",
        "        else:\n",
        "            temp = self.front\n",
        "            while temp.next and temp.next.priority <= priority:\n",
        "                temp = temp.next\n",
        "            newNode.next = temp.next\n",
        "            temp.next = newNode\n",
        "\n",
        "    def pop(self):\n",
        "        if self.isEmpty():\n",
        "            return None\n",
        "        else:\n",
        "            highest_priority_node = self.front\n",
        "            self.front = self.front.next\n",
        "            return highest_priority_node.data\n",
        "\n",
        "# KNN Classifier class\n",
        "class KNNClassifier:\n",
        "    def __init__(self, k):\n",
        "        self.k = k\n",
        "\n",
        "    def calc_dist(self, p1, p2):\n",
        "        return math.sqrt(sum((p1[i] - p2[i])**2 for i in range(len(p1))))\n",
        "\n",
        "    def get_highest_vote(self, predicted_labels):\n",
        "        labels_count = [0] * 3\n",
        "        for label in predicted_labels:\n",
        "            labels_count[label] += 1\n",
        "        return labels_count.index(max(labels_count))\n",
        "\n",
        "    def get_knn_labels(self, k, X_train, y_train, X_point):\n",
        "        labels = PriorityQueue()\n",
        "        for i in range(len(X_train)):\n",
        "            distance = self.calc_dist(X_train[i], X_point)\n",
        "            labels.push(y_train.iloc[i], distance)\n",
        "        knn_labels = [labels.pop() for _ in range(k)]\n",
        "        return knn_labels\n",
        "\n",
        "    def predict(self, X_train, y_train, X_point):\n",
        "        neighbor_labels = self.get_knn_labels(self.k, X_train, y_train, X_point)\n",
        "        return self.get_highest_vote(neighbor_labels)\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def accuracy(y_test, predicted_labels):\n",
        "    correct = sum(pred == true for pred, true in zip(predicted_labels, y_test))\n",
        "    return correct / len(y_test)\n",
        "\n",
        "# Create the KNN classifier\n",
        "knn_clf = KNNClassifier(k)\n",
        "\n",
        "# Predict labels for the test set\n",
        "predicted_labels = [knn_clf.predict(X_train, y_train, X_point) for X_point in X_test]\n",
        "\n",
        "# Print the accuracy\n",
        "print(\"Accuracy:\", accuracy(y_test, predicted_labels))\n"
      ],
      "metadata": {
        "id": "kuedFnHd0e2C",
        "outputId": "84612ac2-4a53-435d-ffd0-56312d28d292",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9629629629629629\n"
          ]
        }
      ]
    }
  ]
}